{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toraaglobal/fashion-mnist/blob/master/02_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-re6y__I_kr",
        "colab_type": "text"
      },
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6RJxMdMKe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "bc21035f-7ce8-49a4-9cfa-b0986966fbba"
      },
      "source": [
        "## Mount the gdrive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "## change directry to the gdrive\n",
        "\n",
        "os.chdir('./drive/My Drive/Colab Notebooks/lab3')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTIKLW2I_kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "\n",
        "## model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier  ##min_samples_split, max_leaf_nodes, max_depth and min_samples_leaf.\n",
        "from sklearn.ensemble import GradientBoostingClassifier  ## The learning_rate is a hyper-parameter in the range (0.0, 1.0] \n",
        "                                                        ##that controls overfitting \n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier  ## clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
        "'''\n",
        "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
        "           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
        "           random_state=None, shuffle=True, tol=0.001,\n",
        "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "from mlp import NeuralNetMLP\n",
        "from mlp import MLPGradientCheck\n",
        "import mlp\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "import struct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ27TgAWI_kw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "386774df-5b24-4e1f-8963-277068711d5c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from kkeras import baseline_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imPqEqoUI_k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "96ed9667-b87d-483f-ffcc-27467bbbe00f"
      },
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
        "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
        "        \n",
        "    with open(labels_path, 'rb') as lbpath:\n",
        "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
        "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
        "\n",
        "    with open(images_path, 'rb') as imgpath:\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
        "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
        " \n",
        "    return images, labels\n",
        "\n",
        "## load traning set\n",
        "X_train, y_train = load_mnist('./data/', kind='train')\n",
        "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
        "\n",
        "## load test set\n",
        "X_test, y_test = load_mnist('./data/', kind='t10k')\n",
        "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows: 60000, columns: 784\n",
            "Rows: 10000, columns: 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbHzk0puI_k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create a model container and emty list to store training and prediction accuracy\n",
        "model = {}\n",
        "model_type = []\n",
        "training_score = []\n",
        "test_score = []\n",
        "training_time = []\n",
        "prediction_time = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68-Wv3SBI_k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# SCRUB\n",
        "# FLATTEN 28 x 28 IMAGE TO 784 VECTOR\n",
        "num_pixels = X_train.shape[1]\n",
        "#X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "#X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "# SCRUB\n",
        "# NORMALIZE INPUTS FROM RGB COLOR TO 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "\n",
        "# SCRUB\n",
        "# THE OLD ONE HOT ENCODE - CONVERT \"CATEGORICAL\" CLASSIFICATION TO ENCODE\n",
        "# A \"BINARIZATION\" OF THE CATEGORIES\n",
        "y_train_k = np_utils.to_categorical(y_train)\n",
        "y_test_k = np_utils.to_categorical(y_test)\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUoRz53xI_k_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "996dcc06-c604-4417-be98-2d7ad1af1a2e"
      },
      "source": [
        "\n",
        "## initialize model and add model to container\n",
        "model['GaussianNB'] = GaussianNB()\n",
        "model['RandomForest'] = RandomForestClassifier()\n",
        "model['GradientBoostingClassifier'] = GradientBoostingClassifier()\n",
        "model['nnMLP'] = NeuralNetMLP(n_output=10, \n",
        "                  n_features=X_train.shape[1], \n",
        "                  n_hidden=50, \n",
        "                  l2=0.1, \n",
        "                  l1=0.0, \n",
        "                  epochs=200, \n",
        "                  eta=0.001,\n",
        "                  alpha=0.001,\n",
        "                  decrease_const=0.00001,\n",
        "                  minibatches=50, \n",
        "                  shuffle=True,\n",
        "                  random_state=1)\n",
        "\n",
        "\n",
        "#model['MLPGradientCheck'] = MLPGradientCheck(n_output=10, \n",
        "#                            n_features=X_train.shape[1], \n",
        "#                            n_hidden=10, \n",
        "#                            l2=0.0, \n",
        "#                            l1=0.0, \n",
        "#                            epochs=100, \n",
        "#                            eta=0.001,\n",
        "#                            alpha=0.0,\n",
        "#                            decrease_const=0.0,\n",
        "#                            minibatches=1, \n",
        "#                            shuffle=False,\n",
        "#                            random_state=1)\n",
        "\n",
        "model['keras_adam'] = baseline_model(num_pixels,num_classes, optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model['keras_sgd'] = baseline_model(num_pixels,num_classes, optimizer='sgd',metrics=['accuracy'])\n",
        "\n",
        "model['keras_Adagrad'] = baseline_model(num_pixels,num_classes, optimizer='Adagrad',metrics=['accuracy'])\n",
        "\n",
        "model['keras_Adadelta'] =baseline_model(num_pixels,num_classes, optimizer='Adadelta',metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pttfvtqI_lC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 30141
        },
        "outputId": "e928c01d-fd57-40a0-b27f-ae59eb6c1135"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "process_start = time.time()\n",
        "for mod in model:\n",
        "    print(\"=\"* 50)\n",
        "    print(mod)\n",
        "    print(\"=\"* 50)\n",
        "\n",
        "    ## start the model\n",
        "    \n",
        "    ## check non keras model\n",
        "    if mod not in ('keras_adam','keras_sgd','keras_Adagrad','keras_Adadelta'):\n",
        "        train_start = time.time()\n",
        "        model[mod].fit(X_train,y_train)\n",
        "        train_end = time.time()\n",
        "    else:\n",
        "        train_start = time.time()\n",
        "        model[mod].fit(X_train,y_train_k, epochs=200)\n",
        "        train_end = time.time()\n",
        "        \n",
        "    \n",
        "    print(\"{} training time {}\".format(mod,train_end-train_start))\n",
        "    \n",
        "    ## train score\n",
        "    if mod not in ('keras_adam','keras_sgd','keras_Adagrad','keras_Adadelta'):\n",
        "        train_prediction = model[mod].predict(X_train)\n",
        "        train_score =  accuracy_score(train_prediction, y_train)\n",
        "    else:\n",
        "        #train_prediction = model[mod].predict(X_train)\n",
        "        train_score =  model[mod].evaluate(X_train, y_train_k)\n",
        "        \n",
        "    print(\"{} training accuracy {}\".format(mod, train_score))\n",
        "    \n",
        "    ## test prediction\n",
        "    if mod not in ('keras_adam','keras_sgd','keras_Adagrad','keras_Adadelta'):\n",
        "        predict_start = time.time()\n",
        "        prediction = model[mod].predict(X_test)\n",
        "        predict_end = time.time()\n",
        "        score = accuracy_score(prediction, y_test)\n",
        "    else:\n",
        "        predict_start = time.time()\n",
        "        #prediction = model[mod].predict(X_test)\n",
        "        score = model[mod].evaluate(X_test, y_test_k)\n",
        "        predict_end = time.time()\n",
        "        \n",
        "     \n",
        "    ## score test\n",
        "    print(\"{} time for testing {}\".format(mod,predict_end-predict_start))\n",
        "    \n",
        "    print(\"{} test accuracy {}\".format(mod, score))\n",
        "    \n",
        "    ## append all results\n",
        "    model_type.append(mod)\n",
        "    training_score.append(train_score)\n",
        "    test_score.append(score)\n",
        "    training_time.append(train_end - train_start)\n",
        "    prediction_time.append(predict_end -  predict_start)\n",
        "    print(\"Done with {}\".format(mod))\n",
        "    \n",
        "\n",
        "process_end = time.time()      \n",
        "## create a dataframe\n",
        "result = {'Model': model_type, 'Training Accuracy': training_score, 'Training Time': training_time, \"Test Accuracy\": test_score,\n",
        "          \"Prediction Time\": prediction_time}\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "print(\"Total Process Time: {}\".format(process_end -  process_start))\n",
        "df\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "GaussianNB\n",
            "==================================================\n",
            "GaussianNB training time 0.7682697772979736\n",
            "GaussianNB training accuracy 0.5877833333333333\n",
            "GaussianNB time for testing 0.6917402744293213\n",
            "GaussianNB test accuracy 0.5856\n",
            "Done with GaussianNB\n",
            "==================================================\n",
            "RandomForest\n",
            "==================================================\n",
            "RandomForest training time 11.132927179336548\n",
            "RandomForest training accuracy 0.9956\n",
            "RandomForest time for testing 0.0856635570526123\n",
            "RandomForest test accuracy 0.8537\n",
            "Done with RandomForest\n",
            "==================================================\n",
            "GradientBoostingClassifier\n",
            "==================================================\n",
            "GradientBoostingClassifier training time 5901.939537763596\n",
            "GradientBoostingClassifier training accuracy 0.9029\n",
            "GradientBoostingClassifier time for testing 0.39362597465515137\n",
            "GradientBoostingClassifier test accuracy 0.8668\n",
            "Done with GradientBoostingClassifier\n",
            "==================================================\n",
            "nnMLP\n",
            "==================================================\n",
            "nnMLP training time 223.91688418388367\n",
            "nnMLP training accuracy 0.9200666666666667\n",
            "nnMLP time for testing 0.09986495971679688\n",
            "nnMLP test accuracy 0.8809\n",
            "Done with nnMLP\n",
            "==================================================\n",
            "keras_adam\n",
            "==================================================\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0263 - acc: 0.9920\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0233 - acc: 0.9928\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0222 - acc: 0.9930\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0250 - acc: 0.9920\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0264 - acc: 0.9919\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0240 - acc: 0.9925\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0192 - acc: 0.9937\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0250 - acc: 0.9923\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0236 - acc: 0.9926\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0238 - acc: 0.9924\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0220 - acc: 0.9928\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0232 - acc: 0.9924\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0260 - acc: 0.9926\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0214 - acc: 0.9935\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0270 - acc: 0.9921\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0201 - acc: 0.9939\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0246 - acc: 0.9924\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0215 - acc: 0.9931\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0248 - acc: 0.9928\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0224 - acc: 0.9934\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0205 - acc: 0.9934\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0246 - acc: 0.9928\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0189 - acc: 0.9939\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0241 - acc: 0.9929\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0222 - acc: 0.9933\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0181 - acc: 0.9941\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0196 - acc: 0.9938\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0216 - acc: 0.9938\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0273 - acc: 0.9921\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0254 - acc: 0.9924\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0159 - acc: 0.9949\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0231 - acc: 0.9929\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0271 - acc: 0.9924\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0205 - acc: 0.9939\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0220 - acc: 0.9934\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0179 - acc: 0.9940\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0202 - acc: 0.9940\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0193 - acc: 0.9940\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0205 - acc: 0.9937\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0214 - acc: 0.9936\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0209 - acc: 0.9938\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0234 - acc: 0.9933\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0196 - acc: 0.9940\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0173 - acc: 0.9949\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0255 - acc: 0.9923\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0213 - acc: 0.9941\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0246 - acc: 0.9932\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0169 - acc: 0.9948\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0217 - acc: 0.9936\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0219 - acc: 0.9934\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0233 - acc: 0.9931\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0198 - acc: 0.9943\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0205 - acc: 0.9940\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0179 - acc: 0.9945\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0175 - acc: 0.9944\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0212 - acc: 0.9940\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0191 - acc: 0.9942\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0253 - acc: 0.9932\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0216 - acc: 0.9936\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0233 - acc: 0.9940\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0158 - acc: 0.9950\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0180 - acc: 0.9946\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0213 - acc: 0.9939\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0189 - acc: 0.9942\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0203 - acc: 0.9941\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0187 - acc: 0.9943\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0229 - acc: 0.9939\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0200 - acc: 0.9947\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0159 - acc: 0.9954\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0203 - acc: 0.9943\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0203 - acc: 0.9943\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0190 - acc: 0.9941\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0178 - acc: 0.9947\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0222 - acc: 0.9938\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0212 - acc: 0.9937\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0176 - acc: 0.9952\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0211 - acc: 0.9941\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0202 - acc: 0.9946\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0166 - acc: 0.9947\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0203 - acc: 0.9945\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0156 - acc: 0.9953\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0164 - acc: 0.9951\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0187 - acc: 0.9944\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0239 - acc: 0.9936\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0196 - acc: 0.9944\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0166 - acc: 0.9950\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0185 - acc: 0.9945\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0185 - acc: 0.9951\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0199 - acc: 0.9945\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0239 - acc: 0.9937\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0149 - acc: 0.9960\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0219 - acc: 0.9941\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0173 - acc: 0.9951\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0182 - acc: 0.9948\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9953\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0197 - acc: 0.9948\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0175 - acc: 0.9946\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0127 - acc: 0.9964\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0249 - acc: 0.9929\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0147 - acc: 0.9955\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0182 - acc: 0.9947\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0193 - acc: 0.9947\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0207 - acc: 0.9940\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0169 - acc: 0.9952\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0206 - acc: 0.9945\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0197 - acc: 0.9944\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0169 - acc: 0.9954\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - acc: 0.9959\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0193 - acc: 0.9947\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0179 - acc: 0.9953\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0180 - acc: 0.9950\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0182 - acc: 0.9948\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0194 - acc: 0.9949\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0176 - acc: 0.9953\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9948\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0206 - acc: 0.9948\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0153 - acc: 0.9958\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9950\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0215 - acc: 0.9940\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0190 - acc: 0.9947\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0241 - acc: 0.9945\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0137 - acc: 0.9962\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0194 - acc: 0.9945\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0121 - acc: 0.9967\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0168 - acc: 0.9953\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0147 - acc: 0.9959\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0215 - acc: 0.9944\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0189 - acc: 0.9948\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0141 - acc: 0.9960\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9949\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0158 - acc: 0.9955\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0129 - acc: 0.9963\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0184 - acc: 0.9946\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0153 - acc: 0.9954\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0196 - acc: 0.9945\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0149 - acc: 0.9958\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0203 - acc: 0.9951\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0145 - acc: 0.9959\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0226 - acc: 0.9946\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0150 - acc: 0.9958\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0186 - acc: 0.9952\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0192 - acc: 0.9945\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0131 - acc: 0.9962\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0162 - acc: 0.9955\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0153 - acc: 0.9954\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0207 - acc: 0.9948\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0126 - acc: 0.9963\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0157 - acc: 0.9952\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0167 - acc: 0.9952\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0144 - acc: 0.9960\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0192 - acc: 0.9951\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0145 - acc: 0.9962\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0174 - acc: 0.9957\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0195 - acc: 0.9950\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0181 - acc: 0.9956\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0165 - acc: 0.9957\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0151 - acc: 0.9962\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0229 - acc: 0.9945\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0142 - acc: 0.9960\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0208 - acc: 0.9943\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0173 - acc: 0.9953\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0179 - acc: 0.9950\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0150 - acc: 0.9958\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0160 - acc: 0.9958\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0176 - acc: 0.9954\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0140 - acc: 0.9961\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0188 - acc: 0.9956\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0215 - acc: 0.9947\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0145 - acc: 0.9959\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0161 - acc: 0.9960\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0168 - acc: 0.9961\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0188 - acc: 0.9951\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0145 - acc: 0.9960\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0151 - acc: 0.9956\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0115 - acc: 0.9969\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0249 - acc: 0.9940\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0183 - acc: 0.9959\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0161 - acc: 0.9959\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - acc: 0.9956\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0160 - acc: 0.9958\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0189 - acc: 0.9956\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0157 - acc: 0.9960\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0178 - acc: 0.9955\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0129 - acc: 0.9965\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0199 - acc: 0.9947\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0154 - acc: 0.9960\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0150 - acc: 0.9961\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0147 - acc: 0.9959\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0181 - acc: 0.9958\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - acc: 0.9957\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0120 - acc: 0.9966\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0233 - acc: 0.9945\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0168 - acc: 0.9958\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0148 - acc: 0.9961\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0171 - acc: 0.9960\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0169 - acc: 0.9957\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0176 - acc: 0.9954\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0191 - acc: 0.9953\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0191 - acc: 0.9957\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0153 - acc: 0.9957\n",
            "keras_adam training time 1177.883723974228\n",
            "60000/60000 [==============================] - 2s 35us/step\n",
            "keras_adam training accuracy [0.011163142539572974, 0.9968333333333333]\n",
            "10000/10000 [==============================] - 0s 34us/step\n",
            "keras_adam time for testing 0.3412792682647705\n",
            "keras_adam test accuracy [1.15368981669005, 0.8943]\n",
            "Done with keras_adam\n",
            "==================================================\n",
            "keras_sgd\n",
            "==================================================\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.6683 - acc: 0.7843\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.4833 - acc: 0.8349\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.4439 - acc: 0.8479\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.4205 - acc: 0.8542\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.4042 - acc: 0.8595\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3905 - acc: 0.8641\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3794 - acc: 0.8679\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3697 - acc: 0.8717\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3602 - acc: 0.8747\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3531 - acc: 0.8775\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3454 - acc: 0.8791\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3394 - acc: 0.8820\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3330 - acc: 0.8838\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3268 - acc: 0.8850\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3216 - acc: 0.8864\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3166 - acc: 0.8876\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3111 - acc: 0.8903\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3066 - acc: 0.8928\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3016 - acc: 0.8938\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2975 - acc: 0.8949\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2929 - acc: 0.8969\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2888 - acc: 0.8981\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2852 - acc: 0.8994\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2816 - acc: 0.9011\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2778 - acc: 0.9021\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2742 - acc: 0.9024\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2700 - acc: 0.9050\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2660 - acc: 0.9054\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2642 - acc: 0.9071\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2609 - acc: 0.9080\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2567 - acc: 0.9089\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2547 - acc: 0.9101\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.2518 - acc: 0.9115\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2485 - acc: 0.9126\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2450 - acc: 0.9136\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2429 - acc: 0.9149\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2397 - acc: 0.9162\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2363 - acc: 0.9173\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2347 - acc: 0.9182\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2315 - acc: 0.9187\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2290 - acc: 0.9199\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2269 - acc: 0.9214\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2241 - acc: 0.9221\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2216 - acc: 0.9227\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2190 - acc: 0.9240\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2171 - acc: 0.9240\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2155 - acc: 0.9248\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.2123 - acc: 0.9265\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2108 - acc: 0.9266\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2080 - acc: 0.9282\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2063 - acc: 0.9280\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2034 - acc: 0.9294\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2013 - acc: 0.9304\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1989 - acc: 0.9316\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1971 - acc: 0.9324\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1943 - acc: 0.9333\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1931 - acc: 0.9336\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1913 - acc: 0.9340\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1883 - acc: 0.9356\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1869 - acc: 0.9358\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1855 - acc: 0.9369\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1837 - acc: 0.9373\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.1810 - acc: 0.9382\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1790 - acc: 0.9388\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1779 - acc: 0.9392\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1755 - acc: 0.9401\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1743 - acc: 0.9415\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1719 - acc: 0.9416\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1703 - acc: 0.9418\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1690 - acc: 0.9424\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1672 - acc: 0.9431\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1656 - acc: 0.9441\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1638 - acc: 0.9442\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1618 - acc: 0.9461\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1603 - acc: 0.9463\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1582 - acc: 0.9472\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1577 - acc: 0.9470\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1552 - acc: 0.9485\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1533 - acc: 0.9493\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1520 - acc: 0.9488\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1502 - acc: 0.9504\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1482 - acc: 0.9506\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1472 - acc: 0.9515\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1453 - acc: 0.9524\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1443 - acc: 0.9519\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1428 - acc: 0.9529\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1417 - acc: 0.9526\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1400 - acc: 0.9543\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1382 - acc: 0.9550\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1363 - acc: 0.9566\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1352 - acc: 0.9564\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1340 - acc: 0.9568\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1326 - acc: 0.9574\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1313 - acc: 0.9571\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1293 - acc: 0.9584\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1287 - acc: 0.9586\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1275 - acc: 0.9590\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1261 - acc: 0.9596\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1251 - acc: 0.9600\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1234 - acc: 0.9612\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1229 - acc: 0.9610\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1208 - acc: 0.9615\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1198 - acc: 0.9612\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1182 - acc: 0.9626\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1169 - acc: 0.9621\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1158 - acc: 0.9636\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1151 - acc: 0.9638\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1137 - acc: 0.9644\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1123 - acc: 0.9650\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1113 - acc: 0.9656\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1092 - acc: 0.9669\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.1094 - acc: 0.9657\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1079 - acc: 0.9673\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1072 - acc: 0.9667\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1058 - acc: 0.9672\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1048 - acc: 0.9678\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1034 - acc: 0.9677\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1026 - acc: 0.9686\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.1010 - acc: 0.9696\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.1000 - acc: 0.9702\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0984 - acc: 0.9700\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0975 - acc: 0.9708\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0970 - acc: 0.9708\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0960 - acc: 0.9714\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0944 - acc: 0.9717\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0939 - acc: 0.9730\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0936 - acc: 0.9724\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0925 - acc: 0.9728\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0909 - acc: 0.9729\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0902 - acc: 0.9738\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0898 - acc: 0.9740\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0878 - acc: 0.9742\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0874 - acc: 0.9742\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0861 - acc: 0.9753\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0854 - acc: 0.9752\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0845 - acc: 0.9752\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0832 - acc: 0.9764\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0828 - acc: 0.9761\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0818 - acc: 0.9762\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0810 - acc: 0.9770\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0807 - acc: 0.9767\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0801 - acc: 0.9772\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0790 - acc: 0.9776\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0779 - acc: 0.9781\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0766 - acc: 0.9790\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0760 - acc: 0.9796\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0754 - acc: 0.9791\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0745 - acc: 0.9798\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0740 - acc: 0.9796\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0728 - acc: 0.9794\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0726 - acc: 0.9802\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0720 - acc: 0.9807\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0717 - acc: 0.9801\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0696 - acc: 0.9815\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0691 - acc: 0.9810\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0687 - acc: 0.9815\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0677 - acc: 0.9819\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0679 - acc: 0.9817\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0655 - acc: 0.9827\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0654 - acc: 0.9829\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0650 - acc: 0.9828\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0641 - acc: 0.9834\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0634 - acc: 0.9836\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0636 - acc: 0.9832\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0618 - acc: 0.9844\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0622 - acc: 0.9839\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0616 - acc: 0.9841\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0611 - acc: 0.9836\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0604 - acc: 0.9846\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0592 - acc: 0.9850\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0582 - acc: 0.9849\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0572 - acc: 0.9859\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0578 - acc: 0.9856\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0562 - acc: 0.9859\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0566 - acc: 0.9859\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0559 - acc: 0.9862\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0546 - acc: 0.9868\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0543 - acc: 0.9872\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0537 - acc: 0.9870\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0531 - acc: 0.9869\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0523 - acc: 0.9877\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0522 - acc: 0.9878\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0519 - acc: 0.9881\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0511 - acc: 0.9878\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0511 - acc: 0.9876\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0494 - acc: 0.9885\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0491 - acc: 0.9889\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0489 - acc: 0.9888\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0482 - acc: 0.9890\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0476 - acc: 0.9888\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0476 - acc: 0.9892\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0467 - acc: 0.9896\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0468 - acc: 0.9892\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0458 - acc: 0.9892\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0451 - acc: 0.9906\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0442 - acc: 0.9907\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0449 - acc: 0.9900\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0428 - acc: 0.9913\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0434 - acc: 0.9910\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0433 - acc: 0.9906\n",
            "keras_sgd training time 946.1553919315338\n",
            "60000/60000 [==============================] - 2s 36us/step\n",
            "keras_sgd training accuracy [0.038993409626434244, 0.9930833333333333]\n",
            "10000/10000 [==============================] - 0s 36us/step\n",
            "keras_sgd time for testing 0.3647294044494629\n",
            "keras_sgd test accuracy [0.3552287856295705, 0.8973]\n",
            "Done with keras_sgd\n",
            "==================================================\n",
            "keras_Adagrad\n",
            "==================================================\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.4811 - acc: 0.8369\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.3514 - acc: 0.8748\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.3201 - acc: 0.8851\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.2990 - acc: 0.8920\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2840 - acc: 0.8967\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2718 - acc: 0.9012\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2613 - acc: 0.9056\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2528 - acc: 0.9086\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2448 - acc: 0.9120\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2373 - acc: 0.9155\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2309 - acc: 0.9185\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.2251 - acc: 0.9197\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2194 - acc: 0.9222\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.2142 - acc: 0.9243\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.2098 - acc: 0.9261\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.2040 - acc: 0.9277\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1999 - acc: 0.9290\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1956 - acc: 0.9325\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1916 - acc: 0.9330\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1880 - acc: 0.9338\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1842 - acc: 0.9360\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1803 - acc: 0.9375\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1772 - acc: 0.9383\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1738 - acc: 0.9399\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1705 - acc: 0.9413\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1679 - acc: 0.9425\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1647 - acc: 0.9438\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1615 - acc: 0.9452\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1587 - acc: 0.9456\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1556 - acc: 0.9474\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1535 - acc: 0.9482\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1509 - acc: 0.9494\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1482 - acc: 0.9506\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1463 - acc: 0.9513\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1437 - acc: 0.9525\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1413 - acc: 0.9538\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1392 - acc: 0.9539\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1372 - acc: 0.9546\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1353 - acc: 0.9556\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1329 - acc: 0.9565\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1309 - acc: 0.9571\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1292 - acc: 0.9582\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1272 - acc: 0.9587\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1254 - acc: 0.9595\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1232 - acc: 0.9603\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1217 - acc: 0.9613\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1198 - acc: 0.9622\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.1184 - acc: 0.9620\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1164 - acc: 0.9630\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1150 - acc: 0.9635\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1135 - acc: 0.9641\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1116 - acc: 0.9657\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1102 - acc: 0.9657\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1087 - acc: 0.9671\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1072 - acc: 0.9672\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1054 - acc: 0.9677\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1044 - acc: 0.9678\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1028 - acc: 0.9689\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1018 - acc: 0.9698\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1001 - acc: 0.9692\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0992 - acc: 0.9704\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0979 - acc: 0.9705\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0966 - acc: 0.9718\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0955 - acc: 0.9718\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0942 - acc: 0.9726\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0931 - acc: 0.9727\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0920 - acc: 0.9733\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0904 - acc: 0.9745\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0892 - acc: 0.9743\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0885 - acc: 0.9747\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0877 - acc: 0.9756\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0864 - acc: 0.9751\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0853 - acc: 0.9760\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0845 - acc: 0.9769\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0832 - acc: 0.9766\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0825 - acc: 0.9775\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0814 - acc: 0.9769\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0804 - acc: 0.9777\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0795 - acc: 0.9788\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0788 - acc: 0.9789\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0777 - acc: 0.9789\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0768 - acc: 0.9795\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0758 - acc: 0.9804\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0749 - acc: 0.9801\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0742 - acc: 0.9807\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0734 - acc: 0.9809\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0727 - acc: 0.9813\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0718 - acc: 0.9813\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0710 - acc: 0.9817\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0701 - acc: 0.9825\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0695 - acc: 0.9823\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0687 - acc: 0.9829\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0678 - acc: 0.9829\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0674 - acc: 0.9828\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0664 - acc: 0.9835\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0658 - acc: 0.9837\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0651 - acc: 0.9838\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0643 - acc: 0.9847\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0636 - acc: 0.9846\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0632 - acc: 0.9847\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0627 - acc: 0.9850\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0618 - acc: 0.9853\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0613 - acc: 0.9857\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0603 - acc: 0.9856\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0601 - acc: 0.9860\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0593 - acc: 0.9863\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0587 - acc: 0.9867\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0582 - acc: 0.9868\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0574 - acc: 0.9872\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0569 - acc: 0.9877\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0564 - acc: 0.9874\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0558 - acc: 0.9877\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0553 - acc: 0.9880\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0548 - acc: 0.9878\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0542 - acc: 0.9885\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0537 - acc: 0.9885\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0534 - acc: 0.9885\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0528 - acc: 0.9889\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0522 - acc: 0.9891\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0517 - acc: 0.9892\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0513 - acc: 0.9893\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0507 - acc: 0.9896\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0501 - acc: 0.9900\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0496 - acc: 0.9899\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0493 - acc: 0.9900\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0489 - acc: 0.9901\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0484 - acc: 0.9905\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0480 - acc: 0.9906\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0474 - acc: 0.9905\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0470 - acc: 0.9906\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0466 - acc: 0.9908\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0461 - acc: 0.9910\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0458 - acc: 0.9911\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0454 - acc: 0.9910\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0450 - acc: 0.9913\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0445 - acc: 0.9914\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0441 - acc: 0.9917\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0437 - acc: 0.9919\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0435 - acc: 0.9915\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0429 - acc: 0.9917\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0426 - acc: 0.9922\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0422 - acc: 0.9920\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0418 - acc: 0.9924\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0415 - acc: 0.9925\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0410 - acc: 0.9925\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0408 - acc: 0.9925\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0404 - acc: 0.9926\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0402 - acc: 0.9930\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0398 - acc: 0.9931\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0395 - acc: 0.9933\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0390 - acc: 0.9935\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0388 - acc: 0.9938\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0383 - acc: 0.9936\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0381 - acc: 0.9934\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0377 - acc: 0.9936\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0375 - acc: 0.9938\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0372 - acc: 0.9938\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0369 - acc: 0.9939\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0365 - acc: 0.9939\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0363 - acc: 0.9938\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0360 - acc: 0.9941\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0358 - acc: 0.9941\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0353 - acc: 0.9941\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0351 - acc: 0.9942\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0348 - acc: 0.9946\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0347 - acc: 0.9944\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0343 - acc: 0.9945\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0340 - acc: 0.9946\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0337 - acc: 0.9949\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0334 - acc: 0.9950\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0332 - acc: 0.9948\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0329 - acc: 0.9949\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0327 - acc: 0.9950\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0323 - acc: 0.9951\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0321 - acc: 0.9951\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0320 - acc: 0.9953\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0316 - acc: 0.9953\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0315 - acc: 0.9953\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0312 - acc: 0.9959\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0311 - acc: 0.9956\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0307 - acc: 0.9957\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0304 - acc: 0.9959\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0303 - acc: 0.9958\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0300 - acc: 0.9958\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0297 - acc: 0.9959\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0296 - acc: 0.9959\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0293 - acc: 0.9961\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0291 - acc: 0.9961\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0290 - acc: 0.9960\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0287 - acc: 0.9963\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0285 - acc: 0.9962\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0282 - acc: 0.9966\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0281 - acc: 0.9964\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0279 - acc: 0.9965\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0277 - acc: 0.9966\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0275 - acc: 0.9965\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0272 - acc: 0.9967\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0271 - acc: 0.9968\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0268 - acc: 0.9968\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0266 - acc: 0.9968\n",
            "keras_Adagrad training time 1005.1570281982422\n",
            "60000/60000 [==============================] - 2s 36us/step\n",
            "keras_Adagrad training accuracy [0.025814523082847398, 0.99695]\n",
            "10000/10000 [==============================] - 0s 34us/step\n",
            "keras_Adagrad time for testing 0.34476518630981445\n",
            "keras_Adagrad test accuracy [0.34931952441707254, 0.9021]\n",
            "Done with keras_Adagrad\n",
            "==================================================\n",
            "keras_Adadelta\n",
            "==================================================\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.4926 - acc: 0.8218\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3616 - acc: 0.8688\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.3249 - acc: 0.8800\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.3031 - acc: 0.8897\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.2826 - acc: 0.8970\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.2709 - acc: 0.9016\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.2588 - acc: 0.9049\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.2472 - acc: 0.9091\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.2386 - acc: 0.9116\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.2286 - acc: 0.9162\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.2191 - acc: 0.9204\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.2145 - acc: 0.9218\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.2047 - acc: 0.9253\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1995 - acc: 0.9276\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1936 - acc: 0.9304\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1887 - acc: 0.9319\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1829 - acc: 0.9341\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1767 - acc: 0.9358\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1723 - acc: 0.9376\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1671 - acc: 0.9394\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1631 - acc: 0.9403\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.1580 - acc: 0.9424\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.1543 - acc: 0.9446\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.1502 - acc: 0.9464\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1457 - acc: 0.9475\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1432 - acc: 0.9491\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1385 - acc: 0.9515\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1349 - acc: 0.9519\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1305 - acc: 0.9541\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1288 - acc: 0.9544\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1246 - acc: 0.9544\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1212 - acc: 0.9568\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1202 - acc: 0.9575\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1165 - acc: 0.9593\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1158 - acc: 0.9599\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1125 - acc: 0.9605\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1093 - acc: 0.9618\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1079 - acc: 0.9631\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1060 - acc: 0.9635\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1017 - acc: 0.9650\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.1003 - acc: 0.9645\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0961 - acc: 0.9671\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0955 - acc: 0.9666\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0941 - acc: 0.9672\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0929 - acc: 0.9682\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0901 - acc: 0.9686\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0879 - acc: 0.9693\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0852 - acc: 0.9705\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0850 - acc: 0.9712\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0816 - acc: 0.9722\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0830 - acc: 0.9717\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0781 - acc: 0.9743\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0777 - acc: 0.9733\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0753 - acc: 0.9745\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0727 - acc: 0.9753\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0742 - acc: 0.9751\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0695 - acc: 0.9763\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0728 - acc: 0.9751\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0686 - acc: 0.9765\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0683 - acc: 0.9769\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0665 - acc: 0.9772\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0646 - acc: 0.9783\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0636 - acc: 0.9783\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0612 - acc: 0.9790\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0612 - acc: 0.9795\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0602 - acc: 0.9804\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0605 - acc: 0.9797\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0584 - acc: 0.9805\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0567 - acc: 0.9809\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0567 - acc: 0.9814\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0534 - acc: 0.9818\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0545 - acc: 0.9817\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0532 - acc: 0.9821\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0529 - acc: 0.9829\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0522 - acc: 0.9827\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0496 - acc: 0.9839\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0485 - acc: 0.9839\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0474 - acc: 0.9840\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0454 - acc: 0.9849\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0462 - acc: 0.9843\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0454 - acc: 0.9847\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0467 - acc: 0.9848\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0442 - acc: 0.9857\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0443 - acc: 0.9855\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0424 - acc: 0.9859\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0436 - acc: 0.9862\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0411 - acc: 0.9872\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0399 - acc: 0.9869\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0403 - acc: 0.9869\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0409 - acc: 0.9864\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0400 - acc: 0.9872\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0390 - acc: 0.9876\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0383 - acc: 0.9877\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0352 - acc: 0.9884\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0370 - acc: 0.9881\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0350 - acc: 0.9883\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0334 - acc: 0.9893\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0332 - acc: 0.9893\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0334 - acc: 0.9892\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0332 - acc: 0.9891\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0327 - acc: 0.9895\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0308 - acc: 0.9894\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0304 - acc: 0.9905\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0315 - acc: 0.9899\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0295 - acc: 0.9906\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0279 - acc: 0.9912\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0293 - acc: 0.9907\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0298 - acc: 0.9906\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0289 - acc: 0.9909\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0293 - acc: 0.9912\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0293 - acc: 0.9906\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0274 - acc: 0.9909\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0265 - acc: 0.9913\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0251 - acc: 0.9920\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0259 - acc: 0.9914\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0242 - acc: 0.9925\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0266 - acc: 0.9917\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0247 - acc: 0.9920\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0243 - acc: 0.9920\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0217 - acc: 0.9931\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0233 - acc: 0.9931\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0227 - acc: 0.9924\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0211 - acc: 0.9934\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0217 - acc: 0.9931\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0202 - acc: 0.9934\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0221 - acc: 0.9929\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0200 - acc: 0.9936\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0188 - acc: 0.9941\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0209 - acc: 0.9937\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0199 - acc: 0.9936\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0190 - acc: 0.9944\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0185 - acc: 0.9940\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0189 - acc: 0.9939\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0188 - acc: 0.9941\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0191 - acc: 0.9943\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0198 - acc: 0.9941\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0187 - acc: 0.9944\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0165 - acc: 0.9950\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0182 - acc: 0.9942\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0174 - acc: 0.9946\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0189 - acc: 0.9944\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0160 - acc: 0.9950\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0165 - acc: 0.9953\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0159 - acc: 0.9949\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0165 - acc: 0.9944\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0157 - acc: 0.9950\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0142 - acc: 0.9957\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0149 - acc: 0.9956\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0137 - acc: 0.9959\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0133 - acc: 0.9960\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0154 - acc: 0.9954\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0133 - acc: 0.9962\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0142 - acc: 0.9955\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0133 - acc: 0.9961\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0131 - acc: 0.9962\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0131 - acc: 0.9961\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0124 - acc: 0.9967\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0136 - acc: 0.9955\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0123 - acc: 0.9962\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0126 - acc: 0.9964\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0124 - acc: 0.9961\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0135 - acc: 0.9957\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0121 - acc: 0.9967\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0109 - acc: 0.9967\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0116 - acc: 0.9966\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0118 - acc: 0.9966\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0103 - acc: 0.9970\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0116 - acc: 0.9968\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0110 - acc: 0.9968\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0114 - acc: 0.9967\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0108 - acc: 0.9967\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0113 - acc: 0.9966\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0096 - acc: 0.9974\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0106 - acc: 0.9971\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0105 - acc: 0.9969\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0094 - acc: 0.9974\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0083 - acc: 0.9976\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0086 - acc: 0.9975\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0087 - acc: 0.9976\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0094 - acc: 0.9971\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0083 - acc: 0.9976\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0077 - acc: 0.9978\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0088 - acc: 0.9974\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0105 - acc: 0.9972\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0084 - acc: 0.9978\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0078 - acc: 0.9973\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0075 - acc: 0.9977\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0076 - acc: 0.9976\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0069 - acc: 0.9981\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0083 - acc: 0.9977\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0072 - acc: 0.9980\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0057 - acc: 0.9985\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0068 - acc: 0.9982\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0070 - acc: 0.9981\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0057 - acc: 0.9986\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0061 - acc: 0.9983\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - acc: 0.9980\n",
            "keras_Adadelta training time 1206.829172372818\n",
            "60000/60000 [==============================] - 2s 36us/step\n",
            "keras_Adadelta training accuracy [0.005669831232304159, 0.9983833333333333]\n",
            "10000/10000 [==============================] - 0s 34us/step\n",
            "keras_Adadelta time for testing 0.34564948081970215\n",
            "keras_Adadelta test accuracy [0.8765296264377539, 0.8993]\n",
            "Done with keras_Adadelta\n",
            "Total Process Time: 10494.964948177338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Prediction Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>1.141622</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.657928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.99515</td>\n",
              "      <td>10.502508</td>\n",
              "      <td>0.8546</td>\n",
              "      <td>0.071329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5638.623037</td>\n",
              "      <td>0.8669</td>\n",
              "      <td>0.621780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.90255</td>\n",
              "      <td>265.021065</td>\n",
              "      <td>0.8688</td>\n",
              "      <td>0.101179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>0.768270</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.691740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>11.132927</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.085664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5901.939538</td>\n",
              "      <td>0.8668</td>\n",
              "      <td>0.393626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.920067</td>\n",
              "      <td>223.916884</td>\n",
              "      <td>0.8809</td>\n",
              "      <td>0.099865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>keras_adam</td>\n",
              "      <td>[0.011163142539572974, 0.9968333333333333]</td>\n",
              "      <td>1177.883724</td>\n",
              "      <td>[1.15368981669005, 0.8943]</td>\n",
              "      <td>0.341279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>keras_sgd</td>\n",
              "      <td>[0.038993409626434244, 0.9930833333333333]</td>\n",
              "      <td>946.155392</td>\n",
              "      <td>[0.3552287856295705, 0.8973]</td>\n",
              "      <td>0.364729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>keras_Adagrad</td>\n",
              "      <td>[0.025814523082847398, 0.99695]</td>\n",
              "      <td>1005.157028</td>\n",
              "      <td>[0.34931952441707254, 0.9021]</td>\n",
              "      <td>0.344765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>keras_Adadelta</td>\n",
              "      <td>[0.005669831232304159, 0.9983833333333333]</td>\n",
              "      <td>1206.829172</td>\n",
              "      <td>[0.8765296264377539, 0.8993]</td>\n",
              "      <td>0.345649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Model  ... Prediction Time\n",
              "0                   GaussianNB  ...        0.657928\n",
              "1                 RandomForest  ...        0.071329\n",
              "2   GradientBoostingClassifier  ...        0.621780\n",
              "3                        nnMLP  ...        0.101179\n",
              "4                   GaussianNB  ...        0.691740\n",
              "5                 RandomForest  ...        0.085664\n",
              "6   GradientBoostingClassifier  ...        0.393626\n",
              "7                        nnMLP  ...        0.099865\n",
              "8                   keras_adam  ...        0.341279\n",
              "9                    keras_sgd  ...        0.364729\n",
              "10               keras_Adagrad  ...        0.344765\n",
              "11              keras_Adadelta  ...        0.345649\n",
              "\n",
              "[12 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh3UP0vmI_lG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "a7dbc931-6d04-4aa7-d7e9-75076b3a8f8a"
      },
      "source": [
        "result = {'Model': model_type, 'Training Accuracy': training_score, 'Training Time': training_time, \"Test Accuracy\": test_score,\n",
        "          \"Prediction Time\": prediction_time}\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "#print(\"Total Process Time: {}\".format(process_end -  process_start))\n",
        "df\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Prediction Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>1.141622</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.657928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.99515</td>\n",
              "      <td>10.502508</td>\n",
              "      <td>0.8546</td>\n",
              "      <td>0.071329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5638.623037</td>\n",
              "      <td>0.8669</td>\n",
              "      <td>0.621780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.90255</td>\n",
              "      <td>265.021065</td>\n",
              "      <td>0.8688</td>\n",
              "      <td>0.101179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>0.768270</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.691740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>11.132927</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.085664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5901.939538</td>\n",
              "      <td>0.8668</td>\n",
              "      <td>0.393626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.920067</td>\n",
              "      <td>223.916884</td>\n",
              "      <td>0.8809</td>\n",
              "      <td>0.099865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>keras_adam</td>\n",
              "      <td>[0.011163142539572974, 0.9968333333333333]</td>\n",
              "      <td>1177.883724</td>\n",
              "      <td>[1.15368981669005, 0.8943]</td>\n",
              "      <td>0.341279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>keras_sgd</td>\n",
              "      <td>[0.038993409626434244, 0.9930833333333333]</td>\n",
              "      <td>946.155392</td>\n",
              "      <td>[0.3552287856295705, 0.8973]</td>\n",
              "      <td>0.364729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>keras_Adagrad</td>\n",
              "      <td>[0.025814523082847398, 0.99695]</td>\n",
              "      <td>1005.157028</td>\n",
              "      <td>[0.34931952441707254, 0.9021]</td>\n",
              "      <td>0.344765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>keras_Adadelta</td>\n",
              "      <td>[0.005669831232304159, 0.9983833333333333]</td>\n",
              "      <td>1206.829172</td>\n",
              "      <td>[0.8765296264377539, 0.8993]</td>\n",
              "      <td>0.345649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Model  ... Prediction Time\n",
              "0                   GaussianNB  ...        0.657928\n",
              "1                 RandomForest  ...        0.071329\n",
              "2   GradientBoostingClassifier  ...        0.621780\n",
              "3                        nnMLP  ...        0.101179\n",
              "4                   GaussianNB  ...        0.691740\n",
              "5                 RandomForest  ...        0.085664\n",
              "6   GradientBoostingClassifier  ...        0.393626\n",
              "7                        nnMLP  ...        0.099865\n",
              "8                   keras_adam  ...        0.341279\n",
              "9                    keras_sgd  ...        0.364729\n",
              "10               keras_Adagrad  ...        0.344765\n",
              "11              keras_Adadelta  ...        0.345649\n",
              "\n",
              "[12 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUjRtif5I_lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8bd0db99-bdb4-4127-98de-68259b0c3db4"
      },
      "source": [
        "result = {'Model': model_type, 'Training Accuracy': training_score, 'Training Time': training_time, \"Test Accuracy\": test_score,\n",
        "          \"Prediction Time\": prediction_time}\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "#print(\"Total Process Time: {}\".format(process_end -  process_start))\n",
        "df\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Prediction Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>1.141622</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.657928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.99515</td>\n",
              "      <td>10.502508</td>\n",
              "      <td>0.8546</td>\n",
              "      <td>0.071329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5638.623037</td>\n",
              "      <td>0.8669</td>\n",
              "      <td>0.621780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.90255</td>\n",
              "      <td>265.021065</td>\n",
              "      <td>0.8688</td>\n",
              "      <td>0.101179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.587783</td>\n",
              "      <td>0.768270</td>\n",
              "      <td>0.5856</td>\n",
              "      <td>0.691740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>11.132927</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.085664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>5901.939538</td>\n",
              "      <td>0.8668</td>\n",
              "      <td>0.393626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>nnMLP</td>\n",
              "      <td>0.920067</td>\n",
              "      <td>223.916884</td>\n",
              "      <td>0.8809</td>\n",
              "      <td>0.099865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>keras_adam</td>\n",
              "      <td>[0.011163142539572974, 0.9968333333333333]</td>\n",
              "      <td>1177.883724</td>\n",
              "      <td>[1.15368981669005, 0.8943]</td>\n",
              "      <td>0.341279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>keras_sgd</td>\n",
              "      <td>[0.038993409626434244, 0.9930833333333333]</td>\n",
              "      <td>946.155392</td>\n",
              "      <td>[0.3552287856295705, 0.8973]</td>\n",
              "      <td>0.364729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>keras_Adagrad</td>\n",
              "      <td>[0.025814523082847398, 0.99695]</td>\n",
              "      <td>1005.157028</td>\n",
              "      <td>[0.34931952441707254, 0.9021]</td>\n",
              "      <td>0.344765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>keras_Adadelta</td>\n",
              "      <td>[0.005669831232304159, 0.9983833333333333]</td>\n",
              "      <td>1206.829172</td>\n",
              "      <td>[0.8765296264377539, 0.8993]</td>\n",
              "      <td>0.345649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Model  ... Prediction Time\n",
              "0                   GaussianNB  ...        0.657928\n",
              "1                 RandomForest  ...        0.071329\n",
              "2   GradientBoostingClassifier  ...        0.621780\n",
              "3                        nnMLP  ...        0.101179\n",
              "4                   GaussianNB  ...        0.691740\n",
              "5                 RandomForest  ...        0.085664\n",
              "6   GradientBoostingClassifier  ...        0.393626\n",
              "7                        nnMLP  ...        0.099865\n",
              "8                   keras_adam  ...        0.341279\n",
              "9                    keras_sgd  ...        0.364729\n",
              "10               keras_Adagrad  ...        0.344765\n",
              "11              keras_Adadelta  ...        0.345649\n",
              "\n",
              "[12 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m88R7yO2I_lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}